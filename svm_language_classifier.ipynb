{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules, Methods, Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import re\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nikkud = ['ֹ', 'ְ', 'ּ', 'ׁ', 'ׂ', 'ָ', 'ֵ', 'ַ', 'ֶ', 'ִ', 'ֻ', 'ֱ', 'ֲ', 'ֳ', 'ׇ']\n",
    "alphabet = ['א','ב','ג','ד','ה','ו','ז','ח','ט','י','כ','ך','ל','מ','ם','נ','ן','ס','ע','פ','ף','צ','ץ','ק','ר','ש','ת']\n",
    "punctuation = ['״', '׳']\n",
    "characters = alphabet + nikkud + punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_to_vec(token, dim):\n",
    "    # print(token)\n",
    "    vec = [0]*dim\n",
    "    for i in range(len(token)):\n",
    "        vec[i * len(characters) + characters.index(token[i])] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(token):\n",
    "    return ''.join([c for c in token if c in characters])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/vowelized_cal_texts/71667_each_training_data.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    data = [{'tag':d['tag'], 'word': clean(d['word'])} for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aramaic words in corpus: 71667\n",
      "Hebrew words in corpus: 71667\n"
     ]
    }
   ],
   "source": [
    "print('Aramaic words in corpus: ' + str(len([w for w in data if w['tag'] == 'A'])))\n",
    "print('Hebrew words in corpus: ' + str(len([w for w in data if w['tag'] == 'R'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:(len(data) * 3 // 4)]\n",
    "test_data = data[(len(data) * 3 //4):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Basic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 20000\n",
    "test_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968\n"
     ]
    }
   ],
   "source": [
    "train_labels = [d['tag'] for d in train_data]\n",
    "test_labels = [d['tag'] for d in test_data]\n",
    "\n",
    "dimension = max([len(d['word']) for d in data]) * len(characters)\n",
    "print(dimension)\n",
    "\n",
    "train_vecs = [tok_to_vec(d['word'], dimension) for d in train_data[:train_size]]\n",
    "test_vecs = [tok_to_vec(d['word'], dimension) for d in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = dimension // 1\n",
    "pca = PCA(pc).fit(train_vecs)\n",
    "train_pcs = pca.transform(train_vecs)\n",
    "test_pcs = pca.transform(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_clf = svm.SVC()#probability=True)\n",
    "lang_clf.fit(train_pcs, train_labels[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9368\n"
     ]
    }
   ],
   "source": [
    "accuracy = sum(np.array(lang_clf.predict(test_pcs[:test_size])) == np.array(test_labels[:test_size])) / test_size\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Talmud Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nazir was not part of the training data\n",
    "with open('./data/aligned_talmud/Nazir.json', encoding='utf-8') as f:\n",
    "    naz = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['בָּעֵי',\n",
       " 'רַבִּי',\n",
       " 'יִרְמְיָה',\n",
       " 'רָקָב',\n",
       " 'הַבָּא',\n",
       " 'מִן',\n",
       " 'הֶעָקֵב',\n",
       " 'מַהוּ',\n",
       " 'כִּי',\n",
       " 'גָּמְרִינַן',\n",
       " 'רָקָב',\n",
       " 'הַבָּא',\n",
       " 'מִכּוּלֵּיהּ',\n",
       " 'מֵת',\n",
       " 'אֲבָל',\n",
       " 'דְּאָתֵי',\n",
       " 'מִן',\n",
       " 'עָקֵב',\n",
       " 'לָא',\n",
       " 'אוֹ',\n",
       " 'דִלְמָא',\n",
       " 'לָא',\n",
       " 'שְׁנָא']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = rd.randrange(len(naz))\n",
    "chunk = rd.randrange(len(naz[page]['content']))\n",
    "words = [word_forms[1] for word_forms in naz[page]['content'][chunk]['text']]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_vecs = [tok_to_vec(word, dimension) for word in words]\n",
    "words_pcs = pca.transform(words_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naz_predictions = lang_clf.predict_proba(words_pcs)\n",
    "naz_predictions = lang_clf.predict(words_pcs)\n",
    "\n",
    "for i in range(len(words)):\n",
    "    print(words[i] + '\\t' + str(naz_predictions[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Saved Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968\n"
     ]
    }
   ],
   "source": [
    "test_labels = [d['tag'] for d in data]\n",
    "\n",
    "dimension = max([len(d['word']) for d in data]) * len(characters)\n",
    "print(dimension)\n",
    "\n",
    "test_vecs = [tok_to_vec(d['word'], dimension) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_clf = joblib.load('./src/languagetagger/GemaraLanguageTagger.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964\n"
     ]
    }
   ],
   "source": [
    "accuracy = sum(np.array(lang_clf.predict(test_vecs[:500])) == np.array(test_labels[:500])) / 500\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Masekhet Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_clf = joblib.load('./src/languagetagger/GemaraLanguageTagger.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/aligned_talmud/Berakhot.json', encoding='utf-8') as f:\n",
    "    mas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['מֵאֵימָתַי',\n",
       " 'קוֹרִין',\n",
       " 'אֶת',\n",
       " 'שְׁמַע',\n",
       " 'בָּעֲרָבִין',\n",
       " 'מִשָּׁעָה',\n",
       " 'שֶׁהַכֹּהֲנִים',\n",
       " 'נִכְנָסִים',\n",
       " 'לֶאֱכוֹל',\n",
       " 'בִּתְרוּמָתָן',\n",
       " 'עַד',\n",
       " 'סוֹף',\n",
       " 'הָאַשְׁמוּרָה',\n",
       " 'הָרִאשׁוֹנָה',\n",
       " 'דִּבְרֵי',\n",
       " 'רַבִּי',\n",
       " 'אֱלִיעֶזֶר']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = 0 #rd.randrange(len(mas))\n",
    "chunk = 0 #rd.randrange(len(mas[page]['content']))\n",
    "words = [word_forms[1] for word_forms in mas[page]['content'][chunk]['text']]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_vecs = [tok_to_vec(word, dimension) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tHebrew      Aramaic\n",
      "מֵאֵימָתַי\t[0.08400024 0.91599976]\n",
      "קוֹרִין\t[0.04981305 0.95018695]\n",
      "אֶת\t[0.0498447 0.9501553]\n",
      "שְׁמַע\t[0.96086188 0.03913812]\n",
      "בָּעֲרָבִין\t[0.11935584 0.88064416]\n",
      "מִשָּׁעָה\t[0.03898098 0.96101902]\n",
      "שֶׁהַכֹּהֲנִים\t[0.02752469 0.97247531]\n",
      "נִכְנָסִים\t[0.01633482 0.98366518]\n",
      "לֶאֱכוֹל\t[0.10155507 0.89844493]\n",
      "בִּתְרוּמָתָן\t[0.23402207 0.76597793]\n",
      "עַד\t[0.04987237 0.95012763]\n",
      "סוֹף\t[0.17321767 0.82678233]\n",
      "הָאַשְׁמוּרָה\t[0.4110613 0.5889387]\n",
      "הָרִאשׁוֹנָה\t[0.04981824 0.95018176]\n",
      "דִּבְרֵי\t[0.04982186 0.95017814]\n",
      "רַבִּי\t[0.04988215 0.95011785]\n",
      "אֱלִיעֶזֶר\t[0.0213943 0.9786057]\n"
     ]
    }
   ],
   "source": [
    "mas_predictions = lang_clf.predict_proba(words_vecs)\n",
    "\n",
    "print('\\t' + 'Hebrew      Aramaic')\n",
    "for i in range(len(words)):\n",
    "    print(words[i] + '\\t' + str(mas_predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
